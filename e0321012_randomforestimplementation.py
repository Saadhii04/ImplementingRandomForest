# -*- coding: utf-8 -*-
"""E0321012_RandomForestImplementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1amPv3gxyy7YsMwRTsQ_H7eP0rPG5LMp1
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Load the dataset
df = pd.read_csv('Cars-RF.csv')

# Rename column names
df.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'acceptability']

# View summary of the dataset
print(df.info())

# Frequency counts of all categorical variables
for col in df.columns:
    if df[col].dtype == 'object':
        print(f"{col}: {df[col].value_counts()}")

# Check for any Null values
print(df.isnull().sum())

from sklearn.model_selection import train_test_split

# split dataset to 60% training and 40% testing
X_train, X_test, y_train, y_test = train_test_split(df.drop(['acceptability'], axis=1), df['acceptability'], test_size=0.4, random_state=0)

# Check the shape of X_train and X_test
print("X_train", X_train.shape)
print("X_test", X_test.shape)

# Check the shape of X_train and X_test
print("X_train:", X_train.shape, "X_test:", X_test.shape)

# Check data types of variables
print(X_train.dtypes)

# Import libraries
from sklearn.preprocessing import LabelEncoder

# Initialize label encoder
le = LabelEncoder()

# Apply label encoding to all categorical variables
X_train = X_train.apply(le.fit_transform)
X_test = X_test.apply(le.fit_transform)

from sklearn.ensemble import RandomForestClassifier

# Initialize Random Forest Classifier with default parameters
rf = RandomForestClassifier()

# Fit the model to the training data
rf.fit(X_train, y_train)

# Initialize Random Forest Classifier with n_estimators = 100
rf_100 = RandomForestClassifier(n_estimators=100)

# Fit the model to the training data
rf_100.fit(X_train, y_train)

# Extract feature importance scores using a trained Random Forest Classifier model
importance_scores = rf_100.feature_importances_

# Create a dataframe to display feature importance scores in descending order
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importance_scores})
feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)

# Display feature importance scores
print(feature_importance_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize feature importance scores to understand the relative importance of different features
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance Scores')
plt.show()

# Select top 3 features based on feature importance scores
selected_features = feature_importance_df['Feature'][:3]

# Create a new dataframe with only the selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Initialize Random Forest Classifier with n_estimators = 100
rf_selected = RandomForestClassifier(n_estimators=100)

# Fit the model to the selected features
rf_selected.fit(X_train_selected, y_train)

from sklearn.metrics import confusion_matrix

# Print the confusion matrix
print(confusion_matrix(y_test, rf_selected.predict(X_test_selected)))

# Compute classification report
classification_report = classification_report(y_test, rf_selected.predict(X_test_selected))

# Display classification report
print(classification_report)